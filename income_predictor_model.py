# -*- coding: utf-8 -*-
"""Income_predictor_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-rGRqD8ziMFsR2f8BrHVbHLLr3HA2m-
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pickle
import os

# Define the column names based on adult.names
COLUMN_NAMES = [
    'age', 'workclass', 'fnlwgt', 'education', 'education-num',
    'marital-status', 'occupation', 'relationship', 'race', 'sex',
    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'
]

#    - header=None: Tells pandas there is no header row in the file.
#    - names=COLUMN_NAMES: Tells pandas to use your list of names as headers.
#    - sep=', ': Tells pandas the data is separated by a comma followed by a space.
#    - skipinitialspace=True: Helps clean up extra spaces.
#    - na_values='?': Instructs pandas to treat all instances of '?' as NaN (Not a Number), which correctly identifies missing values for later imputation.
df_train = pd.read_csv(
    'adult.data',
    header=None,
    names=COLUMN_NAMES,
    sep=',',
    skipinitialspace=True,
    na_values='?'
)

print(df_train.head())

# - Getting dataset dims
df_train.shape

"""### **Q1: What is the extent and distribution of missing and duplicate data across all features, and what is the optimal strategy for imputation (filling in) to maximize model performance?**"""

# - This line shows how many missing values the dataset has for each column
df_train.isnull().sum()

"""The analysis reveals missing data in three categorical features:  workclass  (**5.64%**),  occupation  (**5.66%**), and  native-country  (**1.79%**).

Since the percentage is relatively low, the simpler method for categorical features is ***Mode Imputation*** (replacing the  NaNs  with the most frequent category)
"""

# - This line shows how many duplicated data the dataset has
df_train.duplicated().sum()

"""We can Handle Duplicate Rows with ***Deletion***

Losing  24  rows is a negligible loss of data ( ≈**0.07%**  of your dataset). This loss is statistically insignificant and will not affect your model's training capacity.

### **Q2: What is the ratio of high income ( >50K ) to low income ( ≤50K ) records, and what does this imbalance imply for model evaluation?**
"""

# Counts the frequency of each value for income column
category_count = df_train["income"].value_counts()

plot = category_count.plot.pie(
    autopct='%1.1f%%',  # Display the percentage on each slice
    figsize=(3, 3),     # Set the figure size
    startangle=45,      # Start the first slice at the top
    title='Frequency Comparison of income feature Categories'
)
plt.ylabel('')

"""We will implement the solution when getting to the Step: Model Data (Training and Parameter Tuning)

### **Q3: Is keeping both the education and education-num features redundant, and should one be dropped?**
"""

# Groups all similar education-num rows and calculates the mean for every group
education_mapping = df_train.groupby('education')['education-num'].mean().sort_values(ascending=False).reset_index()
# Renames the column's name
education_mapping.rename(columns={'education-num': 'Average_Education_Num'}, inplace=True)
#Transforms the current index into a new, default integer index starting from 0.
education_mapping = education_mapping.reset_index(drop=True)

# Shows unique education-num feature values
unique_education_num = pd.Series(sorted(df_train['education-num'].unique(), reverse=True), name='Unique_education_num')

education_mapping = pd.concat([education_mapping, unique_education_num], axis=1)
education_mapping.head()

"""The table above shows that Average_Education_Num has exact values of Unique_education_num, which means that education_num feature is an Ordinal encoding of education feature. **We should drop the "education" feature**

### **Q4: Which numerical features exhibit the lowest linear correlation with the  high-income  target, and can these features be removed to simplify the final model?**
"""

# Creates a new binary feature from the original income
df_train['high_income'] = (df_train['income'] == '>50K').astype(int)


numerical_cols = [
    'age', 'fnlwgt', 'education-num',
    'capital-gain', 'capital-loss', 'hours-per-week'
]

# Calculates the Correlation between high-income feature with every other numerical feature
correlation_results = df_train[numerical_cols].corrwith(df_train['high_income']).sort_values(ascending=False)

print("Correlation with High Income Target:")
print(correlation_results)

"""The  fnlwgt  (final weight) feature has a correlation coefficient of **−0.0095** , which is **nearly zero**.

Decision: We should **drop** the  fnlwgt  column.

### **Q5: Which specific  occupation  categories drive the highest probability of earning  >$50K , and based on this distribution, should the low-impact categories be combined via Feature Engineering?**
"""

# Calculating the percentage of high-income earners for each occupation, by using mean of binary values
occupation_impact = df_train.groupby('occupation')['high_income'].mean().sort_values(ascending=False).mul(100).reset_index()

# Plot the result
plt.figure(figsize=(6, 5))
# Use the sorted DataFrame to ensure bars are ordered from highest impact to lowest
sns.barplot(x='occupation', y='high_income', data=occupation_impact, palette='viridis')

# Enhance readability
plt.xticks(rotation=45, ha='right')
plt.title('Percentage of High Income (>$50K) by Occupation', fontsize=14)
plt.ylabel('Percentage Earning >$50K', fontsize=12)
plt.xlabel('Occupation', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

"""**The Multivariate Graphical EDA** on  occupation  revealed that high income ( >$50K ) is concentrated almost entirely in five categories (e.g.,  **Exec-managerial  at  48.4%  and  Prof-specialty  at  44.9%** ). Conversely, the remaining categories, which fall below a  **20%**  high-income threshold, are statistically similar in their low predictive power (e.g.,  **Priv-house-serv  at  0.67%** ).

We will apply **Feature Engineering** by combining these low-impact categories into a single '**Other_Low_Impact**' category. This action reduces model complexity by converting the feature from  **15**  to approximately  **5**  unique categories, preserving the strong predictive signal while dramatically simplifying the final model's required **One-Hot Encoding**.

### **Q6: What is the relationship between the continuous feature  age  and the binary target variable  income ? Specifically, how does the distribution of age differ between the two income groups, and is this difference significant enough to warrant creating a new binary age feature ( is_over_40 )?**
"""

# --- Generate the Box Plot ---
plt.figure(figsize=(7, 5))
# Use the high_income integer column for the x-axis
sns.boxplot(
    x='high_income',
    y='age',
    data=df_train,
    # Palette keys match the integer values 0 and 1 in 'high_income'
    palette={"0": 'blue', "1": 'red'}
)

# Enhance readability
plt.title('Age Distribution by Income Level (Q1)')
plt.xlabel('Income (0: <=50K, 1: >50K)')
plt.ylabel('Age')
plt.xticks([0, 1], ['<=50K', '>50K'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

"""The Box Plot analysis for Q6 ( **Age  vs.  Income** ) clearly indicates that while income overlap exists across all age groups, **the median age for high earners is significantly higher (approximately 38 years old)**.

We will convert the continuous  age  feature into a binary step function. By creating a new feature,  **is_over_38** , we capture the powerful predictive signal of career experience and maturity—the single most important takeaway from the age distribution—while dramatically simplifying the final model's complexity and enhancing its interpretability.
"""

# This function calculates the mode value for each column that has missing values and needs imputation
def calculate_train_imputation_modes():
  IMPUTATION_COLS = ['workclass', 'occupation', 'native-country']
  imputation_modes = {}
  for col in IMPUTATION_COLS:
    mode_value = df_train[col].mode()[0]
    imputation_modes[col] = mode_value
  return imputation_modes

def EDA_Based_Clean(df):
  #------------------------------------Based on Q1----------------------------------------------
  # Load pre-calculated imputation modes
  imputation_modes = calculate_train_imputation_modes()
  # Impute missing values (NaNs) in specified columns using their respective mode values
  for col, mode_val in imputation_modes.items():
    df[col].fillna(mode_val, inplace=True)

  # Remove duplicate rows from the DataFrame
  df.drop_duplicates(inplace=True)

  #------------------------------------Based on Q3----------------------------------------------
  # Drop the 'education' column (redundant with 'education-num')
  df.drop(columns=['education'], inplace=True)

  #------------------------------------Based on Q4----------------------------------------------
  # Drop the 'fnlwgt' column (a population weight often irrelevant for individual prediction)
  df.drop(columns=['fnlwgt'], inplace=True)

  #------------------------------------Based on Q5----------------------------------------------
  # Define a list of occupation categories considered to have low predictive impact
  low_impact_occupations = [
    'Adm-clerical',
    'Machine-op-inspct',
    'Farming-fishing',
    'Armed-Forces',
    'Handlers-cleaners',
    'Other-service',
    'Priv-house-serv'
  ]
  # Define the name for the new, combined category
  new_category_name = 'Other-Low-Impact-occupation'
  # Group the low-impact occupation categories into a single 'Other-Low-Impact-occupation' category
  df['occupation'].replace(
    to_replace= low_impact_occupations,
    value= new_category_name,
    inplace= True
  )

    #------------------------------------Based on Q6----------------------------------------------
  # Define the age threshold for creating a new feature
  AGE_THRESHOLD = 42
  # Convert the 'age' column to numeric, coercing errors to NaN
  df['age'] = pd.to_numeric(df['age'], errors='coerce')
  # Create a new binary feature: 1 if age is >= 42, 0 otherwise
  df['is_over_40'] = (df['age'] >= AGE_THRESHOLD).astype(int)
  # Drop the original 'age' column after creating the binned feature
  df.drop('age', axis=1, inplace=True)


  # Create the binary target variable 'high_income' (1 if '>50K', 0 otherwise)
  df['high_income'] = (df['income'] == '>50K').astype(int)
  # Drop the original 'income' column after creating the binary target
  df.drop(columns=['income'], inplace=True)

  # Identify all columns with object data type (which are typically categorical)
  categorical_cols = df.select_dtypes(include=['object']).columns
  # Perform one-hot encoding on the categorical columns
  df_encoded = pd.get_dummies(
    df,
    columns=categorical_cols,
    drop_first=True  # Avoid multicollinearity by dropping the first category in each feature
  )


  return df_encoded, imputation_modes

def clean_train_data(df_train):

  df__train_encoded, imputation_modes = EDA_Based_Clean(df_train)

  # Separate features (X) from the target variable (y)
  X_train = df__train_encoded.drop('high_income', axis=1)
  # Assign the target variable
  y_train = df__train_encoded['high_income']
 # Store the list of final feature names for later use (e.g., in transforming test data)
  TRAIN_FEATURE_COLUMNS = X_train.columns.tolist()

 # Define the list of numerical columns that need standardization
  numeric_cols = ["education-num", "capital-gain" , "capital-loss", "hours-per-week"]
 # Initialize the StandardScaler object
  scaler = StandardScaler()
 # Fit the scaler to the training data and transform the selected numerical columns
  X_train.loc[:, numeric_cols] = scaler.fit_transform(X_train[numeric_cols])

 # Return the cleaned features, target, feature list, fitted scaler, and numerical column list
  return X_train, y_train, TRAIN_FEATURE_COLUMNS, scaler, numeric_cols, imputation_modes

def clean_test_data(df_test, TRAIN_FEATURE_COLUMNS, fitted_scaler, numeric_cols_to_scale):

  # Clean the 'income' column by stripping whitespace and removing the period
  df_test['income'] = (
    df_test['income']
    .str.strip()
    .str.replace('.', '', regex=False)
  )
  df__test_encoded, imputation_modes = EDA_Based_Clean(df_test)

  # Separate features (X) from the target variable (y)
  X_test = df__test_encoded.drop('high_income', axis=1)
    # Assign the target variable
  y_test = df__test_encoded['high_income']
  # Reindex the test features to match the exact order and presence of training features, filling missing ones with 0
  df_test_final = X_test.reindex(columns=TRAIN_FEATURE_COLUMNS, fill_value=0)

  # Apply the pre-fitted scaler from the training data to the selected numerical columns
  df_test_final[numeric_cols_to_scale] = fitted_scaler.transform(
    df_test_final[numeric_cols_to_scale]
  )

  # Return the final processed test features and the target variable
  return df_test_final, y_test

X_train, y_train, TRAIN_FEATURE_COLUMNS, fitted_scaler, numeric_cols, imputation_modes = clean_train_data(df_train)

# Loading test data
df_test = pd.read_csv(
    'adult.test',
    header=None,
    names=COLUMN_NAMES,
    sep=',', # Based on the snippet, comma separation is used.
    skipinitialspace=True, # Important for cleaning up extra spaces after the comma.
    na_values='?'
)

df_test['income'].value_counts()

df_test_final, y_test_encoded = clean_test_data(df_test, TRAIN_FEATURE_COLUMNS, fitted_scaler, numeric_cols)

df_test['high_income'].value_counts()

model_param = {
    # --- Logistic Regression Configuration ---
    "logistic-regression": {
        # Initialize the Logistic Regression model object
        "model": LogisticRegression(),
        "params": {
            # 'penalty': Regularization norm to use (L1 is Lasso regularization)
            "penalty": ['l1'],
            # 'C': Inverse of regularization strength (smaller C means stronger regularization)
            "C": [0.01, 0.1, 1, 10, 100],
            # 'max_iter': Maximum number of iterations to converge (increased for stability)
            "max_iter": [1000, 2000],
            # 'solver': Algorithm to use for optimization (liblinear supports L1 penalty)
            "solver": ['liblinear']
        }
    },

    # --- Random Forest Configuration ---
    "random-forest": {
        # Initialize the Random Forest Classifier model object
        "model": RandomForestClassifier(),
        "params": {
            # 'n_estimators': Number of trees in the forest
            "n_estimators": [100, 200],
            # 'max_depth': Maximum depth of each tree (limits overfitting)
            "max_depth": [10, 20],
            # 'min_samples_split': Minimum number of samples required to split an internal node
            "min_samples_split": [2, 5, 10],
            # 'max_features': Number of features to consider when looking for the best split
            "max_features": ['sqrt', 'log2']
        }
    },

    # --- K-Nearest Neighbors Configuration ---
    "k-nearest-neighbors": {
        # Initialize the K-Nearest Neighbors Classifier model object
        "model": KNeighborsClassifier(),
        "params": {
            # 'n_neighbors': Number of neighbors to use for classification (k value)
            "n_neighbors": [15, 31],
            # 'weights': Weight function used in prediction ('uniform' or 'distance')
            "weights": ['uniform', 'distance'],
            # 'p': Power parameter for the Minkowski metric (p=1 is Manhattan, p=2 is Euclidean)
            "p": [1, 2]
        }
    }
}

# Note: Assumes model_param is the dictionary provided in the previous turn
def best_model_selector(model_param, X_train, y_train):
    # Initialize a list to store the results from each GridSearch
    scores = []
    # Loop through each model and its parameter grid defined in model_param
    for model_name, mp in model_param.items():
        # Initialize GridSearchCV with the model, parameters, and 5-fold cross-validation
        clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
        # Fit the GridSearchCV object to the training data to find the best parameters
        clf.fit(X_train, y_train)
        # Store the model name, best cross-validation score, and best hyperparameter set
        scores.append({
            'model': model_name,
            'best_score': clf.best_score_,
            'best_params': clf.best_params_
        })
    # Convert the list of results into a pandas DataFrame for comparison
    model_compare = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])
    # Return the row (model) that achieved the maximum 'best_score' (the overall winner)
    return model_compare.loc[model_compare['best_score'].idxmax()]

best_model_info = best_model_selector(model_param, X_train, y_train)

# Show best model to use
best_model_info['best_params']

model = RandomForestClassifier(
    max_depth=20,
    max_features='sqrt',
    min_samples_split=5,
    n_estimators=100,
    random_state=42,
    class_weight='balanced'
)

model.fit(X_train, y_train)

y_probs = model.predict_proba(df_test_final)[:, 1] # Probability of Class 1

# Set a lower threshold
THRESHOLD = 0.35

# Generate new binary predictions based on the threshold
predictions_adjusted = (y_probs > THRESHOLD).astype(int)

# Calculate the overall accuracy of the model predictions
acc = accuracy_score(y_test_encoded, predictions_adjusted)
# Calculate the precision score (ratio of true positive predictions to total positive predictions)
precision = precision_score(y_test_encoded, predictions_adjusted)
# Calculate the recall score (ratio of true positive predictions to actual positives)
recall = recall_score(y_test_encoded, predictions_adjusted)
# Calculate the F1-score, which is the harmonic mean of precision and recall
f1 = f1_score(y_test_encoded, predictions_adjusted)

print(f"Accuracy: {acc:.4f}")
print(f"precision: {precision:.4f}")
print(f"recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Define the desired filename for the trained model
filename = 'income_predection.sav'
# Construct the full path where the model file will be saved
filepath = os.path.join('/content/', filename)

# Use 'with open' to ensure the file is closed and saved successfully
try:
    # Open the specified filepath in binary write mode ('wb')
    with open(filepath, 'wb') as file:
        # Serialize and save the trained model object using pickle
        pickle.dump(model, file)
    # Open a file named 'scaler.pkl' in binary write mode
    with open('scaler.pkl', 'wb') as file:
        # Serialize and save the fitted StandardScaler object
        pickle.dump(fitted_scaler, file)
    # Open a file named 'train_columns.pkl' in binary write mode
    with open('train_columns.pkl', 'wb') as file:
        # Serialize and save the list of training feature column names
        pickle.dump(TRAIN_FEATURE_COLUMNS, file)
    with open('imputation_modes.pkl', 'wb') as file:
        # Serialize and save the list of training feature column names
        pickle.dump(imputation_modes, file)
except Exception as e:
    print(f"An error occurred during saving: {e}")